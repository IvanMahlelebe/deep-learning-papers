{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "In this notebook, I am going to interactively implement a **BERT**(Bidirectional Encoder Representation from Transformer) which is a language representation designed to pretrain deep bidirectional representations from unlabelled text. It does this by jointly conditioning on both left and right context in all of its layers. As a result, after pretraining a BERT model, fine-tuning is done only on the output layer, meaning there's minimal architectural modifications for fine-tuning the model for a specific task.\n",
    "\n",
    "Getting straight into the details, there are two major steps in this framework:\n",
    "* **Pre-training** - This is where the model is pretrained on unlabelled data over different pretraining tasks.\n",
    "* **Fine-tuning** - To do this, the model first initializes with pretrained parameters which are all fine-tuned using labelled data from downstream tasks, thus, as mentioned above, each task will have separate fine-tuned model which were all initialized with the same pretrained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The architecture here is just a variant of a transformer model discussed in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) so we won't discuss it at length, but we'll see as we go, how they're the same.\n",
    "\n",
    "**Model Parameters** - The paper used the following parameters for its base model:\n",
    "* $L = 12$ - Number of layers i.e. the number of stacked Transformer Encoder Blocks, each of which will have sublayers multi-attention mechanism and feed-forward neural network as we already know from previous work.\n",
    "* $H = 768$ - This will be the number of neurons in each hidden state, meaning, after embedding, each token will have a hidden representation of size 768.\n",
    "* $A = 12$ - This indicates the number of self-attention heads in each layer, meaning we'll have $12$ instances of $K, Q, V$ matrices for computing multi-head self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\kmahl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Let's load some data that we'll use for debugging, not necessarily pretraining, just testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "# Note liking where the data is stored and too lazy to fix it\n",
    "def read_corpus(category=\"gold\"):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
    "            n_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    n_corpus_words = -1\n",
    "\n",
    "    corpus_words = list({y for x in corpus for y in x})\n",
    "    n_corpus_words = len(corpus_words)\n",
    "\n",
    "    return corpus_words, n_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()\n",
    "corpus_words, n_corpus_words = distinct_words(reuters_corpus)\n",
    "max_sent_length = max(len(sentence) for sentence in reuters_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2830 unique words in the corpus. The corpus has 124 sentences, where the longest one is of length 836\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {n_corpus_words} unique words in the corpus.\" \\\n",
    "      f\" The corpus has {len(reuters_corpus)} sentences,\" \\\n",
    "      f\" where the longest one is of length {max_sent_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "The model needs to accept a special token $\\text{<CLS>}$ (the first token for every sequence) and a list of word tokens from the user. Each of these tokens are then converted final word embeddings by adding the following embeddings:\n",
    "* **Token embeddings $(TE)$** In order for our model to make something out of words it hasn't seen before(during training) e.g. novel words, mispellings etc, the paper uses `WordPiece` embeddings for each token to create token embedding.\n",
    "* **Segment embeddings $(SE)$** The sentence number encoded in a vector. This is a way to differentiate between sentence A and sentence B e.g. during NSP that we'll discuss shortly.\n",
    "* **Position embeddings $(PE)$** A vector encoding of the position of a word in a sentence.\n",
    "\n",
    "It is thus clear that the segment and position embeddings are both useful in capturing temporal ordering within input sentences.\n",
    "\n",
    "### Implementation details\n",
    "I am not sure how the `WordPiece` algorithm work(and I don't think I have to at this point), so I've found a pretrained model from `Hugging Face` called [`bert-base-uncased`](https://huggingface.co/google-bert/bert-base-uncased), which is the uncased base model of what I'm trying to build, so I'll be using it to encode sentences in my corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "\n",
    "for sentence in reuters_corpus:\n",
    "    encoded_sentence = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_sent_length\n",
    "    )\n",
    "    encoded_sentences.append(encoded_sentence['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([344, 836])\n",
      "torch.Size([344, 836])\n",
      "torch.Size([344, 836])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{encoded_sentence['input_ids'].shape}\\n{encoded_sentence['token_type_ids'].shape}\\n{encoded_sentence['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each sentence is encoded with with 344 I-don't-know-what(maybe indices? I really don't know), and each instance of that is of length 836, which almost makes sense because this is supposed to be the length of each sequence.\n",
    "\n",
    "I'm really not sure whether I'm on the right track or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, maxlen, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.d_model = d_model\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
    "        self.seg_embed = nn.Embedding(2, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            x,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.d_model\n",
    "        )['input_ids']\n",
    "        \n",
    "        seq_len = tokens.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=tokens.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(tokens)\n",
    "        segments = torch.zeros_like(tokens)\n",
    "        token_embeddings = self.bert_model(tokens)[0]\n",
    "        embedding = token_embeddings + self.pos_embed(pos) + self.seg_embed(segments)\n",
    "        return self.norm(embedding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
